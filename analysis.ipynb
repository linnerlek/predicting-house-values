{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config and imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "DATA_PATH = \"Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\"\n",
    "TARGET_STATE = \"GA\"\n",
    "CUTOFF_COL = \"2024-02-29\"      # last column to use for features one year before target\n",
    "TARGET_COL = \"2025-02-28\"      # target column to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4b29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for trimming, growth, and volatility\n",
    "\n",
    "def get_zhvi_columns(df):\n",
    "    return df.columns[df.columns.str.match(r'^\\d{4}-\\d{2}-\\d{2}$')]\n",
    "\n",
    "def longest_nan_streak(row):\n",
    "    is_null = row.isnull()\n",
    "    max_streak = 0\n",
    "    current_streak = 0\n",
    "    for val in is_null:\n",
    "        if val:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 0\n",
    "    return max_streak\n",
    "\n",
    "def trim_to_first_valid(series):\n",
    "    first_valid = series.first_valid_index()\n",
    "    if first_valid:\n",
    "        return series.loc[first_valid:]\n",
    "    return pd.Series(dtype='float64')\n",
    "\n",
    "# calculates percent change from n months ago\n",
    "def yoy_growth(series, months_back=12):\n",
    "    if len(series) < months_back + 1 or series.iloc[-months_back - 1] == 0:\n",
    "        return np.nan\n",
    "    return 100 * (series.iloc[-1] - series.iloc[-months_back - 1]) / series.iloc[-months_back - 1]\n",
    "\n",
    "# calculates total percent growth over entire series\n",
    "def total_growth(series):\n",
    "    if len(series) < 2 or series.iloc[0] == 0:\n",
    "        return np.nan\n",
    "    return 100 * (series.iloc[-1] - series.iloc[0]) / series.iloc[0]\n",
    "\n",
    "# calculates compound annual growth rate\n",
    "def cagr(series):\n",
    "    if len(series) < 2 or series.iloc[0] <= 0:\n",
    "        return np.nan\n",
    "    years = len(series) / 12\n",
    "    return ((series.iloc[-1] / series.iloc[0]) ** (1 / years) - 1) * 100\n",
    "\n",
    "# calculates average monthly percent change\n",
    "def avg_monthly_growth(series):\n",
    "    monthly_pct = series.pct_change()\n",
    "    return 100 * monthly_pct.mean()\n",
    "\n",
    "# calculates standard deviation of monthly percent changes\n",
    "def volatility(series):\n",
    "    monthly_pct = series.pct_change()\n",
    "    return 100 * monthly_pct.std()\n",
    "\n",
    "# calculates yoy growth for each full year with available data\n",
    "def compute_yoy_by_year(series):\n",
    "    series = series.copy()\n",
    "    series.index = pd.to_datetime(series.index)\n",
    "    yearly_growth = {}\n",
    "\n",
    "    for year in range(series.index.year.min() + 1, series.index.year.max() + 1):\n",
    "        try:\n",
    "            prev = series[series.index.year == year - 1].iloc[-1]\n",
    "            curr = series[series.index.year == year].iloc[-1]\n",
    "            if pd.notna(prev) and prev != 0:\n",
    "                growth = 100 * (curr - prev) / prev\n",
    "                yearly_growth[year] = growth\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    return pd.Series(yearly_growth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68225229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 665 ZIPs from GA\n",
      "\n",
      "missing metadata breakdown:\n",
      "missing both city and metro: 11\n",
      "missing only city: 25\n",
      "missing only metro: 111\n",
      "\n",
      "metadata cleanup:\n",
      "rows before drop: 658\n",
      "rows after drop: 558\n",
      "rows removed: 100\n",
      "\n",
      "trimmed zhvi series for 558 ZIPs\n",
      "\n",
      "interpolation result:\n",
      "clean rows: 558\n",
      "removed rows: 0\n"
     ]
    }
   ],
   "source": [
    "# loads dataset, fills metadata gaps, trims zhvi, interpolates missing values\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df[df[\"State\"] == TARGET_STATE].copy()\n",
    "print(f\"loaded {df.shape[0]} ZIPs from {TARGET_STATE}\")\n",
    "\n",
    "zhvi_cols = get_zhvi_columns(df)\n",
    "\n",
    "# check initial missing metadata counts\n",
    "meta_cols = [\"RegionID\", \"RegionName\", \"City\", \"Metro\", \"CountyName\", \"SizeRank\", \"State\"]\n",
    "missing_meta = df[meta_cols].isnull().sum()\n",
    "missing_meta = missing_meta[missing_meta > 0]\n",
    "both_missing = df[df[\"City\"].isnull() & df[\"Metro\"].isnull()].shape[0]\n",
    "only_city_missing = df[df[\"City\"].isnull() & df[\"Metro\"].notnull()].shape[0]\n",
    "only_metro_missing = df[df[\"Metro\"].isnull() & df[\"City\"].notnull()].shape[0]\n",
    "\n",
    "print(\"\\nmissing metadata breakdown:\")\n",
    "print(f\"missing both city and metro: {both_missing}\")\n",
    "print(f\"missing only city: {only_city_missing}\")\n",
    "print(f\"missing only metro: {only_metro_missing}\")\n",
    "\n",
    "# fill metro using city → metro mapping, avoiding ambiguous cities\n",
    "ambiguous_city_set = {\"Boston\", \"Tifton\"}\n",
    "safe_city_to_metro = (\n",
    "    df[df[\"City\"].notna() & df[\"Metro\"].notna() & ~df[\"City\"].isin(ambiguous_city_set)]\n",
    "    .groupby(\"City\")[\"Metro\"]\n",
    "    .agg(lambda x: x.mode().iloc[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df[\"Metro\"] = df.apply(\n",
    "    lambda row: safe_city_to_metro.get(row[\"City\"], row[\"Metro\"]) if pd.isna(row[\"Metro\"]) else row[\"Metro\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# fill remaining metro using county to metro mapping\n",
    "county_to_metro = (\n",
    "    df[df[\"Metro\"].notna()][[\"CountyName\", \"Metro\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby(\"CountyName\")[\"Metro\"]\n",
    "    .agg(lambda x: x.mode().iloc[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df[\"Metro\"] = df.apply(\n",
    "    lambda row: county_to_metro.get(row[\"CountyName\"], row[\"Metro\"]) if pd.isna(row[\"Metro\"]) else row[\"Metro\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# drop rows missing both city and metro, then drop rows missing either\n",
    "df = df[~(df[\"City\"].isnull() & df[\"Metro\"].isnull())].copy()\n",
    "before_drop = df.shape[0]\n",
    "df = df[df[\"City\"].notna() & df[\"Metro\"].notna()].copy()\n",
    "after_drop = df.shape[0]\n",
    "\n",
    "print(f\"\\nmetadata cleanup:\")\n",
    "print(f\"rows before drop: {before_drop}\")\n",
    "print(f\"rows after drop: {after_drop}\")\n",
    "print(f\"rows removed: {before_drop - after_drop}\")\n",
    "\n",
    "# ========== ✅ NEW: calculate YoY_2025 BEFORE trimming ==========\n",
    "def compute_target_yoy(row, cutoff=CUTOFF_COL, target=TARGET_COL):\n",
    "    try:\n",
    "        prev = row[cutoff]\n",
    "        curr = row[target]\n",
    "        if pd.notna(prev) and prev != 0:\n",
    "            return 100 * (curr - prev) / prev\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return np.nan\n",
    "\n",
    "df[\"YoY_2025\"] = df.apply(lambda row: compute_target_yoy(row), axis=1)\n",
    "\n",
    "# trim zhvi series to first valid value and up to cutoff\n",
    "cutoff_cols = [col for col in zhvi_cols if col <= CUTOFF_COL]\n",
    "trimmed_series_list, trimmed_lengths, missing_counts = [], [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trimmed = trim_to_first_valid(row[cutoff_cols]).astype(\"float64\")\n",
    "    trimmed_series_list.append(trimmed)\n",
    "    trimmed_lengths.append(len(trimmed))\n",
    "    missing_counts.append(trimmed.isnull().sum())\n",
    "\n",
    "df[\"TrimmedZHVI\"] = trimmed_series_list\n",
    "df[\"TrimmedLength\"] = trimmed_lengths\n",
    "df[\"TrimmedMissing\"] = missing_counts\n",
    "\n",
    "print(f\"\\ntrimmed zhvi series for {len(df)} ZIPs\")\n",
    "\n",
    "# interpolate missing values within each trimmed series\n",
    "interpolated_series, interpolated_missing_counts = [], []\n",
    "\n",
    "for s in df[\"TrimmedZHVI\"]:\n",
    "    interpolated = s.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "    interpolated_series.append(interpolated)\n",
    "    interpolated_missing_counts.append(interpolated.isna().sum())\n",
    "\n",
    "df[\"InterpolatedZHVI\"] = interpolated_series\n",
    "df[\"InterpolatedMissing\"] = interpolated_missing_counts\n",
    "\n",
    "before_interp = df.shape[0]\n",
    "df = df[df[\"InterpolatedMissing\"] == 0].copy()\n",
    "after_interp = df.shape[0]\n",
    "\n",
    "print(f\"\\ninterpolation result:\")\n",
    "print(f\"clean rows: {after_interp}\")\n",
    "print(f\"removed rows: {before_interp - after_interp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b9fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zhvi tier distribution:\n",
      "ZHVI_Tier\n",
      "high          22\n",
      "low          128\n",
      "mid          323\n",
      "very high     18\n",
      "very low      67\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# computes engineered features using interpolated zhvi\n",
    "\n",
    "# final zhvi is the value at the cutoff\n",
    "df[\"FinalZHVI\"] = df[\"InterpolatedZHVI\"].apply(lambda s: s.loc[CUTOFF_COL] if CUTOFF_COL in s.index else np.nan)\n",
    "\n",
    "# z-score across all final values\n",
    "df[\"FinalZScore\"] = zscore(df[\"FinalZHVI\"])\n",
    "\n",
    "# assign value tier using asymmetric z-score bins\n",
    "def zhvi_asymmetric_tier(z):\n",
    "    if z >= 2.5:\n",
    "        return \"very high\"\n",
    "    elif z >= 1.5:\n",
    "        return \"high\"\n",
    "    elif z >= -0.5:\n",
    "        return \"mid\"\n",
    "    elif z >= -1.0:\n",
    "        return \"low\"\n",
    "    else:\n",
    "        return \"very low\"\n",
    "\n",
    "df[\"ZHVI_Tier\"] = df[\"FinalZScore\"].apply(zhvi_asymmetric_tier)\n",
    "\n",
    "# compute yoy growth per year (EXCLUDING 2025 to avoid leakage)\n",
    "yoy_df = df[\"InterpolatedZHVI\"].apply(lambda s: compute_yoy_by_year(s.loc[:CUTOFF_COL]))\n",
    "yoy_df = yoy_df.add_prefix(\"YoY_\")\n",
    "df = pd.concat([df, yoy_df], axis=1)\n",
    "\n",
    "# compute aggregate features from yearly growth\n",
    "df[\"AvgYoYGrowth\"] = yoy_df.mean(axis=1)\n",
    "df[\"MedianYoYGrowth\"] = yoy_df.median(axis=1)\n",
    "df[\"YoYGrowthVolatility\"] = yoy_df.std(axis=1)\n",
    "df[\"NegativeGrowthYears\"] = (yoy_df < 0).sum(axis=1)\n",
    "\n",
    "# compute full-period features using the interpolated series\n",
    "df[\"Total_Growth\"] = df[\"InterpolatedZHVI\"].apply(total_growth)\n",
    "df[\"CAGR\"] = df[\"InterpolatedZHVI\"].apply(cagr)\n",
    "df[\"AvgMonthlyGrowth\"] = df[\"InterpolatedZHVI\"].apply(avg_monthly_growth)\n",
    "df[\"Volatility\"] = df[\"InterpolatedZHVI\"].apply(volatility)\n",
    "\n",
    "# rename RegionName to ZIP for clarity\n",
    "df.rename(columns={\"RegionName\": \"ZIP\"}, inplace=True)\n",
    "\n",
    "# show distribution of value tiers\n",
    "print(\"zhvi tier distribution:\")\n",
    "print(df[\"ZHVI_Tier\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f209d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged ACS ZIP-level features. Missing ZIP matches: 0\n"
     ]
    }
   ],
   "source": [
    "# ========== Pull and merge ACS ZIP-level features (Demographic, Housing, Education, etc.) ==========\n",
    "\n",
    "import requests\n",
    "\n",
    "API_KEY = \"b06c64cd8eadc4fa292ce05e788144d98223fa3f\"\n",
    "\n",
    "# ACS variable codes to fetch\n",
    "acs_variables = [\n",
    "    # housing + vehicles\n",
    "    \"B25003_001E\", \"B25003_003E\",     # total housing, renter-occupied\n",
    "    \"B08201_001E\", \"B08201_002E\",     # total households, no vehicle\n",
    "\n",
    "    # education\n",
    "    \"B15003_001E\",                   # education total\n",
    "    \"B15003_022E\", \"B15003_023E\", \"B15003_024E\", \"B15003_025E\",  # degrees\n",
    "\n",
    "    # employment and poverty\n",
    "    \"B23025_003E\", \"B23025_005E\",    # labor force, unemployed\n",
    "    \"B17001_001E\", \"B17001_002E\",    # poverty total, below poverty\n",
    "\n",
    "    # age breakdown\n",
    "    \"B01003_001E\",  # total population\n",
    "    \"B01001_003E\", \"B01001_027E\",  # age <5 (male/female)\n",
    "    \"B01001_004E\", \"B01001_028E\",  # age 5-9\n",
    "    \"B01001_005E\", \"B01001_029E\",  # age 10-14\n",
    "    \"B01001_006E\", \"B01001_030E\",  # age 15-17\n",
    "    \"B01001_007E\", \"B01001_031E\",  # age 18-19\n",
    "    \"B01001_008E\", \"B01001_032E\",  # age 20-24\n",
    "    \"B01001_009E\", \"B01001_033E\",  # age 25-29\n",
    "    \"B01001_010E\", \"B01001_034E\",  # age 30-34\n",
    "    \"B01001_020E\", \"B01001_044E\",  # age 65+\n",
    "\n",
    "    # household types\n",
    "    \"B11001_001E\", \"B11001_008E\",   # total households, 1-person\n",
    "    \"B11016_001E\", \"B11016_010E\"    # total households, 4+ people\n",
    "]\n",
    "\n",
    "# build API URL\n",
    "base_url = \"https://api.census.gov/data/2023/acs/acs5\"\n",
    "get_vars = \",\".join([\"NAME\"] + acs_variables)\n",
    "acs_url = f\"{base_url}?get={get_vars}&for=zip%20code%20tabulation%20area:*&key={API_KEY}\"\n",
    "\n",
    "# fetch and parse\n",
    "response = requests.get(acs_url)\n",
    "data = response.json()\n",
    "columns = data[0]\n",
    "rows = data[1:]\n",
    "acs_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# rename for clarity\n",
    "acs_df.rename(columns={\n",
    "    \"zip code tabulation area\": \"ZIP\",\n",
    "    \"B25003_001E\": \"Total_Housing\",\n",
    "    \"B25003_003E\": \"Renter_Occupied\",\n",
    "    \"B08201_001E\": \"Total_Households_Vehicle\",\n",
    "    \"B08201_002E\": \"No_Vehicle_Households\",\n",
    "    \"B15003_001E\": \"Education_Total\",\n",
    "    \"B15003_022E\": \"Bachelors\",\n",
    "    \"B15003_023E\": \"Masters\",\n",
    "    \"B15003_024E\": \"Professional\",\n",
    "    \"B15003_025E\": \"Doctorate\",\n",
    "    \"B23025_003E\": \"Labor_Force\",\n",
    "    \"B23025_005E\": \"Unemployed\",\n",
    "    \"B17001_001E\": \"Poverty_Total\",\n",
    "    \"B17001_002E\": \"Below_Poverty\",\n",
    "    \"B01003_001E\": \"Population_Total\",\n",
    "    \"B01001_003E\": \"Age_U5_M\",  \"B01001_027E\": \"Age_U5_F\",\n",
    "    \"B01001_004E\": \"Age_5_9_M\", \"B01001_028E\": \"Age_5_9_F\",\n",
    "    \"B01001_005E\": \"Age_10_14_M\", \"B01001_029E\": \"Age_10_14_F\",\n",
    "    \"B01001_006E\": \"Age_15_17_M\", \"B01001_030E\": \"Age_15_17_F\",\n",
    "    \"B01001_007E\": \"Age_18_19_M\", \"B01001_031E\": \"Age_18_19_F\",\n",
    "    \"B01001_008E\": \"Age_20_24_M\", \"B01001_032E\": \"Age_20_24_F\",\n",
    "    \"B01001_009E\": \"Age_25_29_M\", \"B01001_033E\": \"Age_25_29_F\",\n",
    "    \"B01001_010E\": \"Age_30_34_M\", \"B01001_034E\": \"Age_30_34_F\",\n",
    "    \"B01001_020E\": \"Age_65p_M\", \"B01001_044E\": \"Age_65p_F\",\n",
    "    \"B11001_001E\": \"Household_Total\",\n",
    "    \"B11001_008E\": \"One_Person_Household\",\n",
    "    \"B11016_001E\": \"Households_SizeTotal\",\n",
    "    \"B11016_010E\": \"Households_4plus\"\n",
    "}, inplace=True)\n",
    "\n",
    "# convert to numeric\n",
    "for col in acs_df.columns:\n",
    "    if col not in [\"ZIP\", \"NAME\"]:\n",
    "        acs_df[col] = pd.to_numeric(acs_df[col], errors=\"coerce\")\n",
    "\n",
    "# engineered percentage features\n",
    "acs_df[\"Pct_Renter_Occupied\"] = 100 * acs_df[\"Renter_Occupied\"] / acs_df[\"Total_Housing\"]\n",
    "acs_df[\"Pct_Bachelors_Or_Higher\"] = 100 * (\n",
    "    acs_df[\"Bachelors\"] + acs_df[\"Masters\"] + acs_df[\"Professional\"] + acs_df[\"Doctorate\"]\n",
    ") / acs_df[\"Education_Total\"]\n",
    "acs_df[\"Unemployment_Rate\"] = 100 * acs_df[\"Unemployed\"] / acs_df[\"Labor_Force\"]\n",
    "acs_df[\"Pct_Below_Poverty\"] = 100 * acs_df[\"Below_Poverty\"] / acs_df[\"Poverty_Total\"]\n",
    "acs_df[\"Pct_No_Vehicle\"] = 100 * acs_df[\"No_Vehicle_Households\"] / acs_df[\"Total_Households_Vehicle\"]\n",
    "acs_df[\"Pct_One_Person_HH\"] = 100 * acs_df[\"One_Person_Household\"] / acs_df[\"Household_Total\"]\n",
    "acs_df[\"Pct_4plus_HH\"] = 100 * acs_df[\"Households_4plus\"] / acs_df[\"Households_SizeTotal\"]\n",
    "acs_df[\"Pct_Age_0_17\"] = 100 * (\n",
    "    acs_df[\"Age_U5_M\"] + acs_df[\"Age_U5_F\"] +\n",
    "    acs_df[\"Age_5_9_M\"] + acs_df[\"Age_5_9_F\"] +\n",
    "    acs_df[\"Age_10_14_M\"] + acs_df[\"Age_10_14_F\"] +\n",
    "    acs_df[\"Age_15_17_M\"] + acs_df[\"Age_15_17_F\"]\n",
    ") / acs_df[\"Population_Total\"]\n",
    "acs_df[\"Pct_Age_18_34\"] = 100 * (\n",
    "    acs_df[\"Age_18_19_M\"] + acs_df[\"Age_18_19_F\"] +\n",
    "    acs_df[\"Age_20_24_M\"] + acs_df[\"Age_20_24_F\"] +\n",
    "    acs_df[\"Age_25_29_M\"] + acs_df[\"Age_25_29_F\"] +\n",
    "    acs_df[\"Age_30_34_M\"] + acs_df[\"Age_30_34_F\"]\n",
    ") / acs_df[\"Population_Total\"]\n",
    "acs_df[\"Pct_Age_65plus\"] = 100 * (\n",
    "    acs_df[\"Age_65p_M\"] + acs_df[\"Age_65p_F\"]\n",
    ") / acs_df[\"Population_Total\"]\n",
    "\n",
    "# final subset\n",
    "final_acs = acs_df[[\n",
    "    \"ZIP\", \"Pct_Renter_Occupied\", \"Pct_Bachelors_Or_Higher\", \"Unemployment_Rate\",\n",
    "    \"Pct_Below_Poverty\", \"Pct_No_Vehicle\", \"Pct_One_Person_HH\", \"Pct_4plus_HH\",\n",
    "    \"Pct_Age_0_17\", \"Pct_Age_18_34\", \"Pct_Age_65plus\"\n",
    "]].copy()\n",
    "final_acs[\"ZIP\"] = final_acs[\"ZIP\"].astype(str)\n",
    "\n",
    "# merge into main df\n",
    "df[\"ZIP\"] = df[\"ZIP\"].astype(str)\n",
    "df = df.merge(final_acs, on=\"ZIP\", how=\"left\")\n",
    "\n",
    "# check merge results\n",
    "missing_zips = df[\"Pct_Renter_Occupied\"].isna().sum()\n",
    "print(f\"✅ Merged ACS ZIP-level features. Missing ZIP matches: {missing_zips}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3bd5c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final dataset preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIP</th>\n",
       "      <th>Metro</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>StateSizeRank</th>\n",
       "      <th>FinalZHVI</th>\n",
       "      <th>FinalZScore</th>\n",
       "      <th>ZHVI_Tier</th>\n",
       "      <th>YoY_2001</th>\n",
       "      <th>YoY_2002</th>\n",
       "      <th>YoY_2003</th>\n",
       "      <th>...</th>\n",
       "      <th>Pct_Bachelors_Or_Higher</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>Pct_Below_Poverty</th>\n",
       "      <th>Pct_No_Vehicle</th>\n",
       "      <th>Pct_One_Person_HH</th>\n",
       "      <th>Pct_4plus_HH</th>\n",
       "      <th>Pct_Age_0_17</th>\n",
       "      <th>Pct_Age_18_34</th>\n",
       "      <th>Pct_Age_65plus</th>\n",
       "      <th>YoY_2025</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ZIP, Metro, CountyName, StateSizeRank, FinalZHVI, FinalZScore, ZHVI_Tier, YoY_2001, YoY_2002, YoY_2003, YoY_2004, YoY_2005, YoY_2006, YoY_2007, YoY_2008, YoY_2009, YoY_2010, YoY_2011, YoY_2012, YoY_2013, YoY_2014, YoY_2015, YoY_2016, YoY_2017, YoY_2018, YoY_2019, YoY_2020, YoY_2021, YoY_2022, YoY_2023, YoY_2024, AvgYoYGrowth, MedianYoYGrowth, YoYGrowthVolatility, NegativeGrowthYears, Pct_Renter_Occupied, Pct_Bachelors_Or_Higher, Unemployment_Rate, Pct_Below_Poverty, Pct_No_Vehicle, Pct_One_Person_HH, Pct_4plus_HH, Pct_Age_0_17, Pct_Age_18_34, Pct_Age_65plus, YoY_2025]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported cleaned dataset with 558 rows and 46 columns\n"
     ]
    }
   ],
   "source": [
    "# prepares final export dataset and writes to csv\n",
    "\n",
    "# assign new state-specific rank based on original SizeRank values\n",
    "df[\"StateSizeRank\"] = df[\"SizeRank\"].rank(method=\"min\").astype(int)\n",
    "\n",
    "# drop RegionID and original SizeRank\n",
    "df.drop(columns=[\"RegionID\", \"SizeRank\"], inplace=True)\n",
    "\n",
    "# columns to drop\n",
    "drop_cols = [\n",
    "    col for col in df.columns\n",
    "    if col in [\n",
    "        \"State\", \"StateName\", \"RegionType\", \"TrimmedZHVI\", \"InterpolatedZHVI\",\n",
    "        \"TrimmedLength\", \"TrimmedMissing\", \"InterpolatedMissing\",\n",
    "        \"Total_Growth\", \"CAGR\", \"AvgMonthlyGrowth\", \"Volatility\", \"City\"\n",
    "    ] or col in zhvi_cols\n",
    "]\n",
    "\n",
    "# create final cleaned version for modeling\n",
    "export_df = df.drop(columns=drop_cols)\n",
    "\n",
    "# reorder columns\n",
    "meta_cols = [\"ZIP\", \"Metro\", \"CountyName\", \"StateSizeRank\"]\n",
    "target_col = \"YoY_2025\"\n",
    "feature_cols = [col for col in export_df.columns if col not in meta_cols + [target_col]]\n",
    "ordered_cols = meta_cols + feature_cols + [target_col]\n",
    "export_df = export_df[ordered_cols]\n",
    "\n",
    "# shuffle rows\n",
    "export_df = export_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# export to csv\n",
    "print(\"final dataset preview:\")\n",
    "display(export_df.head(0))\n",
    "export_df.to_csv(\"final_processed_dataset.csv\", index=False)\n",
    "print(f\"exported cleaned dataset with {export_df.shape[0]} rows and {export_df.shape[1]} columns\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
