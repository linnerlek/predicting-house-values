{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config and imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "DATA_PATH = \"Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\"\n",
    "TARGET_STATE = \"GA\"\n",
    "CUTOFF_COL = \"2024-02-29\"      # last column to use for features one year before target\n",
    "TARGET_COL = \"2025-02-28\"      # target column to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4b29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for trimming, growth, and volatility\n",
    "\n",
    "def get_zhvi_columns(df):\n",
    "    return df.columns[df.columns.str.match(r'^\\d{4}-\\d{2}-\\d{2}$')]\n",
    "\n",
    "def longest_nan_streak(row):\n",
    "    is_null = row.isnull()\n",
    "    max_streak = 0\n",
    "    current_streak = 0\n",
    "    for val in is_null:\n",
    "        if val:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 0\n",
    "    return max_streak\n",
    "\n",
    "def trim_to_first_valid(series):\n",
    "    first_valid = series.first_valid_index()\n",
    "    if first_valid:\n",
    "        return series.loc[first_valid:]\n",
    "    return pd.Series(dtype='float64')\n",
    "\n",
    "# calculates percent change from n months ago\n",
    "def yoy_growth(series, months_back=12):\n",
    "    if len(series) < months_back + 1 or series.iloc[-months_back - 1] == 0:\n",
    "        return np.nan\n",
    "    return 100 * (series.iloc[-1] - series.iloc[-months_back - 1]) / series.iloc[-months_back - 1]\n",
    "\n",
    "# calculates total percent growth over entire series\n",
    "def total_growth(series):\n",
    "    if len(series) < 2 or series.iloc[0] == 0:\n",
    "        return np.nan\n",
    "    return 100 * (series.iloc[-1] - series.iloc[0]) / series.iloc[0]\n",
    "\n",
    "# calculates compound annual growth rate\n",
    "def cagr(series):\n",
    "    if len(series) < 2 or series.iloc[0] <= 0:\n",
    "        return np.nan\n",
    "    years = len(series) / 12\n",
    "    return ((series.iloc[-1] / series.iloc[0]) ** (1 / years) - 1) * 100\n",
    "\n",
    "# calculates average monthly percent change\n",
    "def avg_monthly_growth(series):\n",
    "    monthly_pct = series.pct_change()\n",
    "    return 100 * monthly_pct.mean()\n",
    "\n",
    "# calculates standard deviation of monthly percent changes\n",
    "def volatility(series):\n",
    "    monthly_pct = series.pct_change()\n",
    "    return 100 * monthly_pct.std()\n",
    "\n",
    "# calculates yoy growth for each full year with available data\n",
    "def compute_yoy_by_year(series):\n",
    "    series = series.copy()\n",
    "    series.index = pd.to_datetime(series.index)\n",
    "    yearly_growth = {}\n",
    "\n",
    "    for year in range(series.index.year.min() + 1, series.index.year.max() + 1):\n",
    "        try:\n",
    "            prev = series[series.index.year == year - 1].iloc[-1]\n",
    "            curr = series[series.index.year == year].iloc[-1]\n",
    "            if pd.notna(prev) and prev != 0:\n",
    "                growth = 100 * (curr - prev) / prev\n",
    "                yearly_growth[year] = growth\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    return pd.Series(yearly_growth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68225229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 665 ZIPs from GA\n",
      "\n",
      "missing metadata breakdown:\n",
      "missing both city and metro: 11\n",
      "missing only city: 25\n",
      "missing only metro: 111\n",
      "\n",
      "metadata cleanup:\n",
      "rows before drop: 658\n",
      "rows after drop: 558\n",
      "rows removed: 100\n",
      "\n",
      "trimmed zhvi series for 558 ZIPs\n",
      "\n",
      "interpolation result:\n",
      "clean rows: 558\n",
      "removed rows: 0\n"
     ]
    }
   ],
   "source": [
    "# loads dataset, fills metadata gaps, trims zhvi, interpolates missing values\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df[df[\"State\"] == TARGET_STATE].copy()\n",
    "print(f\"loaded {df.shape[0]} ZIPs from {TARGET_STATE}\")\n",
    "\n",
    "zhvi_cols = get_zhvi_columns(df)\n",
    "\n",
    "# check initial missing metadata counts\n",
    "meta_cols = [\"RegionID\", \"RegionName\", \"City\", \"Metro\", \"CountyName\", \"SizeRank\", \"State\"]\n",
    "missing_meta = df[meta_cols].isnull().sum()\n",
    "missing_meta = missing_meta[missing_meta > 0]\n",
    "\n",
    "both_missing = df[df[\"City\"].isnull() & df[\"Metro\"].isnull()].shape[0]\n",
    "only_city_missing = df[df[\"City\"].isnull() & df[\"Metro\"].notnull()].shape[0]\n",
    "only_metro_missing = df[df[\"Metro\"].isnull() & df[\"City\"].notnull()].shape[0]\n",
    "\n",
    "print(\"\\nmissing metadata breakdown:\")\n",
    "print(f\"missing both city and metro: {both_missing}\")\n",
    "print(f\"missing only city: {only_city_missing}\")\n",
    "print(f\"missing only metro: {only_metro_missing}\")\n",
    "\n",
    "# fill metro using city â†’ metro mapping, avoiding ambiguous cities\n",
    "ambiguous_city_set = {\"Boston\", \"Tifton\"}\n",
    "safe_city_to_metro = (\n",
    "    df[df[\"City\"].notna() & df[\"Metro\"].notna() & ~df[\"City\"].isin(ambiguous_city_set)]\n",
    "    .groupby(\"City\")[\"Metro\"]\n",
    "    .agg(lambda x: x.mode().iloc[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df[\"Metro\"] = df.apply(\n",
    "    lambda row: safe_city_to_metro.get(row[\"City\"], row[\"Metro\"]) if pd.isna(row[\"Metro\"]) else row[\"Metro\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# fill remaining metro using county to metro mapping\n",
    "county_to_metro = (\n",
    "    df[df[\"Metro\"].notna()][[\"CountyName\", \"Metro\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby(\"CountyName\")[\"Metro\"]\n",
    "    .agg(lambda x: x.mode().iloc[0])\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df[\"Metro\"] = df.apply(\n",
    "    lambda row: county_to_metro.get(row[\"CountyName\"], row[\"Metro\"]) if pd.isna(row[\"Metro\"]) else row[\"Metro\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# drop rows missing both city and metro, then drop rows missing either\n",
    "df = df[~(df[\"City\"].isnull() & df[\"Metro\"].isnull())].copy()\n",
    "before_drop = df.shape[0]\n",
    "df = df[df[\"City\"].notna() & df[\"Metro\"].notna()].copy()\n",
    "after_drop = df.shape[0]\n",
    "\n",
    "print(f\"\\nmetadata cleanup:\")\n",
    "print(f\"rows before drop: {before_drop}\")\n",
    "print(f\"rows after drop: {after_drop}\")\n",
    "print(f\"rows removed: {before_drop - after_drop}\")\n",
    "\n",
    "# trim zhvi series to first valid value and up to cutoff\n",
    "cutoff_cols = [col for col in zhvi_cols if col <= CUTOFF_COL]\n",
    "trimmed_series_list, trimmed_lengths, missing_counts = [], [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trimmed = trim_to_first_valid(row[cutoff_cols]).astype(\"float64\")\n",
    "    trimmed_series_list.append(trimmed)\n",
    "    trimmed_lengths.append(len(trimmed))\n",
    "    missing_counts.append(trimmed.isnull().sum())\n",
    "\n",
    "df[\"TrimmedZHVI\"] = trimmed_series_list\n",
    "df[\"TrimmedLength\"] = trimmed_lengths\n",
    "df[\"TrimmedMissing\"] = missing_counts\n",
    "\n",
    "print(f\"\\ntrimmed zhvi series for {len(df)} ZIPs\")\n",
    "\n",
    "# interpolate missing values within each trimmed series\n",
    "interpolated_series, interpolated_missing_counts = [], []\n",
    "for s in df[\"TrimmedZHVI\"]:\n",
    "    interpolated = s.interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "    interpolated_series.append(interpolated)\n",
    "    interpolated_missing_counts.append(interpolated.isna().sum())\n",
    "\n",
    "df[\"InterpolatedZHVI\"] = interpolated_series\n",
    "df[\"InterpolatedMissing\"] = interpolated_missing_counts\n",
    "\n",
    "before_interp = df.shape[0]\n",
    "df = df[df[\"InterpolatedMissing\"] == 0].copy()\n",
    "after_interp = df.shape[0]\n",
    "\n",
    "print(f\"\\ninterpolation result:\")\n",
    "print(f\"clean rows: {after_interp}\")\n",
    "print(f\"removed rows: {before_interp - after_interp}\")\n",
    "\n",
    "# define target as the most recent zhvi value in the full series\n",
    "df[TARGET_COL] = df[\"InterpolatedZHVI\"].apply(lambda s: s.iloc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b9fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zhvi tier distribution:\n",
      "ZHVI_Tier\n",
      "high          22\n",
      "low          128\n",
      "mid          323\n",
      "very high     18\n",
      "very low      67\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# computes engineered features using interpolated zhvi\n",
    "\n",
    "# final zhvi is the last value in the trimmed series\n",
    "df[\"FinalZHVI\"] = df[\"InterpolatedZHVI\"].apply(lambda s: s.iloc[-1])\n",
    "\n",
    "# z-score across all final values\n",
    "df[\"FinalZScore\"] = zscore(df[\"FinalZHVI\"])\n",
    "\n",
    "# assign value tier using asymmetric z-score bins\n",
    "def zhvi_asymmetric_tier(z):\n",
    "    if z >= 2.5:\n",
    "        return \"very high\"\n",
    "    elif z >= 1.5:\n",
    "        return \"high\"\n",
    "    elif z >= -0.5:\n",
    "        return \"mid\"\n",
    "    elif z >= -1.0:\n",
    "        return \"low\"\n",
    "    else:\n",
    "        return \"very low\"\n",
    "\n",
    "df[\"ZHVI_Tier\"] = df[\"FinalZScore\"].apply(zhvi_asymmetric_tier)\n",
    "\n",
    "# compute yoy growth per year from the interpolated series\n",
    "yoy_df = df[\"InterpolatedZHVI\"].apply(compute_yoy_by_year)\n",
    "yoy_df = yoy_df.add_prefix(\"YoY_\")\n",
    "df = pd.concat([df, yoy_df], axis=1)\n",
    "\n",
    "# compute aggregate features from the yearly growth\n",
    "df[\"AvgYoYGrowth\"] = yoy_df.mean(axis=1)\n",
    "df[\"MedianYoYGrowth\"] = yoy_df.median(axis=1)\n",
    "df[\"YoYGrowthVolatility\"] = yoy_df.std(axis=1)\n",
    "df[\"NegativeGrowthYears\"] = (yoy_df < 0).sum(axis=1)\n",
    "\n",
    "# compute full-period features using the interpolated series\n",
    "df[\"Total_Growth\"] = df[\"InterpolatedZHVI\"].apply(total_growth)\n",
    "df[\"CAGR\"] = df[\"InterpolatedZHVI\"].apply(cagr)\n",
    "df[\"AvgMonthlyGrowth\"] = df[\"InterpolatedZHVI\"].apply(avg_monthly_growth)\n",
    "df[\"Volatility\"] = df[\"InterpolatedZHVI\"].apply(volatility)\n",
    "\n",
    "# show distribution of value tiers\n",
    "print(\"zhvi tier distribution:\")\n",
    "print(df[\"ZHVI_Tier\"].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bd5c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final dataset preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIP</th>\n",
       "      <th>City</th>\n",
       "      <th>Metro</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>StateSizeRank</th>\n",
       "      <th>FinalZHVI</th>\n",
       "      <th>FinalZScore</th>\n",
       "      <th>ZHVI_Tier</th>\n",
       "      <th>AvgYoYGrowth</th>\n",
       "      <th>MedianYoYGrowth</th>\n",
       "      <th>YoYGrowthVolatility</th>\n",
       "      <th>NegativeGrowthYears</th>\n",
       "      <th>Total_Growth</th>\n",
       "      <th>CAGR</th>\n",
       "      <th>AvgMonthlyGrowth</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>2025-02-28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ZIP, City, Metro, CountyName, StateSizeRank, FinalZHVI, FinalZScore, ZHVI_Tier, AvgYoYGrowth, MedianYoYGrowth, YoYGrowthVolatility, NegativeGrowthYears, Total_Growth, CAGR, AvgMonthlyGrowth, Volatility, 2025-02-28]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported cleaned dataset with 558 rows and 17 columns\n"
     ]
    }
   ],
   "source": [
    "# prepares final export dataset and writes to csv\n",
    "\n",
    "# assign new state-specific rank based on original SizeRank values\n",
    "df[\"StateSizeRank\"] = df[\"SizeRank\"].rank(method=\"min\").astype(int)\n",
    "\n",
    "# drop RegionID and original SizeRank\n",
    "df.drop(columns=[\"RegionID\", \"SizeRank\"], inplace=True)\n",
    "\n",
    "# rename RegionName to ZIP for clarity\n",
    "df.rename(columns={\"RegionName\": \"ZIP\"}, inplace=True)\n",
    "\n",
    "# columns to drop \n",
    "drop_cols = [\n",
    "    col for col in df.columns\n",
    "    if (col in [\n",
    "        \"State\", \"StateName\", \"RegionType\", \"TrimmedZHVI\", \"InterpolatedZHVI\",\n",
    "        \"TrimmedLength\", \"TrimmedMissing\", \"InterpolatedMissing\"\n",
    "    ] or (col in zhvi_cols and col != TARGET_COL) or col.startswith(\"YoY_\")) \n",
    "]\n",
    "\n",
    "# create final cleaned version for modeling\n",
    "export_df = df.drop(columns=drop_cols)\n",
    "\n",
    "# reorder columns \n",
    "meta_cols = [\"ZIP\", \"City\", \"Metro\", \"CountyName\", \"StateSizeRank\"]\n",
    "target_col = TARGET_COL\n",
    "feature_cols = [col for col in export_df.columns if col not in meta_cols + [target_col]]\n",
    "ordered_cols = meta_cols + feature_cols + [target_col]\n",
    "export_df = export_df[ordered_cols]\n",
    "\n",
    "# shuffle rows\n",
    "export_df = export_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# export to csv\n",
    "print(\"final dataset preview:\")\n",
    "display(export_df.head(0))\n",
    "\n",
    "export_df.to_csv(\"final_processed_dataset.csv\", index=False)\n",
    "print(f\"exported cleaned dataset with {export_df.shape[0]} rows and {export_df.shape[1]} columns\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
